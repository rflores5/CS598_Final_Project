{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vsqqQW6rh13S","outputId":"cdf43cc9-24f8-44f9-d7ab-3d9fc725c4d0"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\mkumari\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\mkumari\\AppData\\Roaming\\nltk_data...\n","C:\\Users\\mkumari\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from collections import defaultdict\n","import math\n","import re\n","import gensim\n","from gensim.models import Word2Vec\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, auc, f1_score, precision_score, recall_score, roc_auc_score\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","pd.set_option('display.max_colwidth', -1)"]},{"cell_type":"markdown","metadata":{"id":"NXQNJf_Eh13W"},"source":["# Data Pre-processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"84jm7vssh13f"},"outputs":[],"source":["data = pd.read_csv ('top_50_notes.csv',encoding='utf-8')\n","stop_words = set(stopwords.words('english'))\n","\n","\n","#data cleaning steps\n","for i in range(len(data)):\n","\n","    # Converting to Lowercase\n","    data.loc[i,'NOTE']  = data.loc[i,'NOTE'] .lower()\n","\n","    #removing all text within square bracket\n","    data.loc[i, 'NOTE'] = re.sub(r\"\\[.*?\\]\", \"\", data.loc[i,'NOTE'] , flags=re.I)\n","   \n","    #removing all square bracket\n","    data.loc[i, 'NOTE'] = re.sub(r\"[\\[\\]]+\", \"\", data.loc[i,'NOTE'] , flags=re.I)\n","\n","    #Remove the new line characters\n","    data.loc[i,'NOTE'] = re.sub(r\"\\t|\\n|\\r\", \" \", data.loc[i,'NOTE'] , flags=re.I)\n","\n","    #Remove punctuation\n","    data.loc[i,'NOTE'] = re.sub(r\"[,‘@\\#-:'?\\.$%_!()&;+”/…*•|“]\", \" \", data.loc[i,'NOTE'] , flags=re.I)\n","\n","    #Remove duble quotes\n","    data.loc[i,'NOTE'] = re.sub(r'\"', \" \", data.loc[i,'NOTE'] , flags=re.I)\n","\n","    #Remove digits\n","    data.loc[i,'NOTE'] = re.sub(r\"\\d\", \"\", data.loc[i,'NOTE'] )\n","\n","    #Remove all single characters\n","    data.loc[i,'NOTE'] = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', data.loc[i,'NOTE'] )\n","\n","    #Substituting multiple spaces with single space\n","    data.loc[i,'NOTE']  = re.sub(r'\\s+', ' ', data.loc[i,'NOTE'] , flags=re.I)\n","   \n","    #remove stopwords\n","    words = data.loc[i,'NOTE'].split()  #Split the text into words\n","    filtered_words = [word for word in words if word not in stop_words]  #Filter out stop words\n","    data.loc[i,'NOTE'] = \" \".join(filtered_words) # Join the filtered words back into a text\n","   \n","    #Remove all single characters\n","    data.loc[i,'NOTE'] = re.sub(r'\\s+[a-zA-Z]\\s+', '', data.loc[i,'NOTE'] )\n","   \n","    # Lemmatize words\n","    lemmatizer = WordNetLemmatizer()\n","    words = data.loc[i,'NOTE'].split()  # Split the text into words\n","    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize each word\n","    data.loc[i,'NOTE'] = \" \".join(lemmatized_words) # Join the lemmatized words back into a text\n","   \n","   \n","\n","#After Pre-processing saving the file\n","data.to_csv('data_preporssing_05042023.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"kXnQm1Q6h13i"},"source":["# Word2Vec Word Embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sMP8cx2Ph13i"},"outputs":[],"source":["# Load your DataFrame with diagnosis ID and symptoms columns\n","df = pd.read_csv('data_preporssing_05042023.csv',encoding='utf-8')\n","\n","# Tokenize the symptoms column\n","df['NOTE'] = df['NOTE'].apply(lambda x: word_tokenize(x.lower()))\n","\n","# Train Word2Vec model\n","model = Word2Vec(df['NOTE'], sg=1, min_count=10, vector_size=128, window=5, workers=4)\n","\n","#Save the model\n","model.save('word2Vec_embeddings_05042023.bin')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u7hFrPcuh13j"},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}