{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gFemeQb3nlc9","executionInfo":{"status":"ok","timestamp":1683329863986,"user_tz":240,"elapsed":255,"user":{"displayName":"Manisha Karn","userId":"01042289321905919897"}},"outputId":"ea2e39df-7df6-4133-c358-a762b91a7d7d"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","<ipython-input-16-508ef5c754fb>:30: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n","  pd.set_option('display.max_colwidth', -1)\n"]}],"source":["from sklearn.metrics import *\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from collections import defaultdict\n","import math\n","\n","import pandas as pd\n","import numpy as np\n","from collections import defaultdict\n","import math\n","import re\n","import gensim\n","from gensim.models import Word2Vec\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, auc, f1_score, precision_score, recall_score, roc_auc_score\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","pd.set_option('display.max_colwidth', -1)"]},{"cell_type":"code","source":["\"\"\"\n","Used when run through google colab\n","from google.colab import drive\n","drive.mount('/content/drive')\n","file_path = '/content/drive/MyDrive/Colab Notebooks/data_preporssing_05042023.csv'\n","\"\"\"\n","\n","file_path = 'data_preporssing_05042023.csv'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0JsSqrrkoSNT","executionInfo":{"status":"ok","timestamp":1683329870208,"user_tz":240,"elapsed":1361,"user":{"displayName":"Manisha Karn","userId":"01042289321905919897"}},"outputId":"764fabbf-0e71-4950-a34e-0aea8a5206ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q3BcaK50nldB"},"outputs":[],"source":["# Load CSV file containing text and labels\n","df = pd.read_csv(file_path)\n","\n","top_50_diag = df['DIAG'].unique()\n","top_50_dict = {}\n","num_of_classes=len(df['DIAG'].unique())\n","\n","for i in range(len(top_50_diag)):\n","    top_50_dict[top_50_diag[i]] = i\n","\n","def y_vec(diag):\n","    vec = np.zeros((50,))\n","    idx = top_50_dict[diag]\n","    vec[idx] = 1\n","    return vec\n","\n","df[\"Y_Var\"]= df[\"DIAG\"].apply(lambda x: y_vec(x))\n","\n","# Split the data into training and testing sets\n","train_df, test_df = train_test_split(df, test_size=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V2q8bCtunldB"},"outputs":[],"source":["# Load pre-trained word2vec model\n","# saved_word2vec_model = '/content/drive/MyDrive/Colab Notebooks/word2Vec_embeddings_05042023.bin'\n","saved_word2vec_model = 'word2Vec_embeddings_05042023.bin'\n","word2vec_model = gensim.models.Word2Vec.load(saved_word2vec_model)\n","\n","# Set parameters\n","embedding_dim = word2vec_model.vector_size\n","hidden_dim = 100\n","vocab_size = len(word2vec_model.wv)\n","\n","# Create embedding matrix\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","for i, word in enumerate(word2vec_model.wv.index_to_key):\n","    embedding_matrix[i] = word2vec_model.wv[word]\n","    \n","\n","# Tokenize text using word2vec model vocabulary for training set\n","tokenized_train = train_df['NOTE'].apply(lambda x: [word2vec_model.wv.key_to_index[word] for word in x.split() if word in word2vec_model.wv.key_to_index])\n","\n","# Tokenize text using word2vec model vocabulary for testing set\n","tokenized_test = test_df['NOTE'].apply(lambda x: [word2vec_model.wv.key_to_index[word] for word in x.split() if word in word2vec_model.wv.key_to_index])\n","\n","# Set maximum sequence length\n","max_seq_length = 400\n","\n","# Pad sequences to the same length for training set\n","padded_train = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(i[:max_seq_length]) for i in tokenized_train], batch_first=True)\n","\n","# Pad sequences to the same length for testing set\n","padded_test = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(i[:max_seq_length]) for i in tokenized_test], batch_first=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uUHVx2rUnldC"},"outputs":[],"source":["X_train=padded_train\n","X_test=padded_test\n","Y_train = torch.from_numpy(np.asarray(train_df['Y_Var'].tolist(), dtype=np.float32))\n","Y_test = torch.from_numpy(np.asarray(test_df['Y_Var'].tolist(), dtype=np.float32))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOZNoVIfnldD"},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = torch.nn.LSTM(embedding_dim, hidden_size=100,  num_layers=2, dropout=0.8, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2 ,  num_of_classes)\n","        self.sigmoid = torch.nn.Sigmoid()\n","\n","    def forward(self, x):\n","        #print(x.shape)\n","        embedded = self.embedding(x)\n","        #print(embedded.shape)\n","        output, (hidden_states, cell_states) = self.lstm(embedded)\n","        #print(output.shape)\n","        output = self.fc(output)\n","        #print(output.shape)\n","        output = torch.sum(output, dim=1)\n","        #print(output.shape)\n","        return self.sigmoid(output)\n","\n","\n","model = Net()"]},{"cell_type":"code","source":["print(X_train.shape)\n","print(X_test.shape)"],"metadata":{"id":"9ZuAyd3gRo6l"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k3CO8uoVnldE"},"outputs":[],"source":["criterion = None\n","optimizer = None\n","\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sm3OD2s1nldE"},"outputs":[],"source":["def classification_metrics(Y_score, Y_pred, Y_true):\n","    acc, auc, precision, recall, f1score = accuracy_score(Y_true, Y_pred), \\\n","                                           roc_auc_score(Y_true, Y_score), \\\n","                                           precision_score(Y_true, Y_pred), \\\n","                                           recall_score(Y_true, Y_pred), \\\n","                                           f1_score(Y_true, Y_pred)\n","    return acc, auc, precision, recall, f1score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sroP633PnldE"},"outputs":[],"source":["def evaluate(model, X, Y):\n","    model.eval()\n","    all_y_true = torch.LongTensor()\n","    all_y_pred = torch.LongTensor()\n","    all_y_score = torch.FloatTensor()\n","    y_hat = model(X)\n","    for i in range(len(X)):\n","        y_pred = (y_hat[i] > 0.2).int()\n","        all_y_true = torch.cat((all_y_true, Y[i].to('cpu')), dim=0)\n","        all_y_pred = torch.cat((all_y_pred,  y_pred.to('cpu')), dim=0)\n","        all_y_score = torch.cat((all_y_score,  y_hat[i].to('cpu')), dim=0)\n","        \n","    acc, auc, precision, recall, f1 = classification_metrics(all_y_score.detach().numpy(), \n","                                                              all_y_pred.detach().numpy(), \n","                                                              all_y_true.detach().numpy())\n","    print(f\"acc: {acc:.3f}, auc: {auc:.3f}, precision: {precision:.3f}, recall: {recall:.3f}, f1: {f1:.3f}\")\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CVSNvrkOnldF","executionInfo":{"status":"ok","timestamp":1683310710429,"user_tz":240,"elapsed":17798,"user":{"displayName":"Manisha Karn","userId":"01042289321905919897"}},"outputId":"db78155e-07fc-4fdb-e636-4dfda7b63f32"},"outputs":[{"output_type":"stream","name":"stdout","text":["model perfomance before training:\n","acc: 0.496, auc: 0.458, precision: 0.018, recall: 0.458, f1: 0.035\n"]}],"source":["print(\"model perfomance before training:\")\n","#evaluate(model, X_train, Y_train)\n","evaluate(model, X_test, Y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y1Z4Jk7JnldF","executionInfo":{"status":"ok","timestamp":1683320566699,"user_tz":240,"elapsed":2913743,"user":{"displayName":"Manisha Karn","userId":"01042289321905919897"}},"outputId":"b655a424-70c7-48ef-ba4b-727dba2fdaa4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1 \tTraining Loss: 0.015927\n","Test Accuracy:\n","acc: 0.918, auc: 0.619, precision: 0.039, recall: 0.133, f1: 0.060\n","Epoch: 2 \tTraining Loss: 0.010672\n","Test Accuracy:\n","acc: 0.934, auc: 0.684, precision: 0.048, recall: 0.124, f1: 0.070\n","Epoch: 3 \tTraining Loss: 0.010526\n","Test Accuracy:\n","acc: 0.937, auc: 0.713, precision: 0.069, recall: 0.174, f1: 0.099\n","Epoch: 4 \tTraining Loss: 0.010424\n","Test Accuracy:\n","acc: 0.939, auc: 0.746, precision: 0.088, recall: 0.221, f1: 0.126\n","Epoch: 5 \tTraining Loss: 0.010316\n","Test Accuracy:\n","acc: 0.939, auc: 0.810, precision: 0.106, recall: 0.272, f1: 0.152\n","Epoch: 6 \tTraining Loss: 0.010249\n","Test Accuracy:\n","acc: 0.940, auc: 0.831, precision: 0.119, recall: 0.313, f1: 0.173\n","Epoch: 7 \tTraining Loss: 0.010214\n","Test Accuracy:\n","acc: 0.939, auc: 0.848, precision: 0.135, recall: 0.375, f1: 0.199\n","Epoch: 8 \tTraining Loss: 0.010155\n","Test Accuracy:\n","acc: 0.939, auc: 0.870, precision: 0.144, recall: 0.413, f1: 0.213\n","Epoch: 9 \tTraining Loss: 0.009226\n","Test Accuracy:\n","acc: 0.959, auc: 0.871, precision: 0.199, recall: 0.356, f1: 0.255\n","Epoch: 10 \tTraining Loss: 0.005146\n","Test Accuracy:\n","acc: 0.958, auc: 0.902, precision: 0.228, recall: 0.450, f1: 0.302\n","Epoch: 11 \tTraining Loss: 0.005136\n","Test Accuracy:\n","acc: 0.958, auc: 0.909, precision: 0.236, recall: 0.480, f1: 0.316\n","Epoch: 12 \tTraining Loss: 0.005130\n","Test Accuracy:\n","acc: 0.959, auc: 0.917, precision: 0.243, recall: 0.506, f1: 0.328\n","Epoch: 13 \tTraining Loss: 0.005123\n","Test Accuracy:\n","acc: 0.959, auc: 0.921, precision: 0.248, recall: 0.529, f1: 0.338\n","Epoch: 14 \tTraining Loss: 0.005115\n","Test Accuracy:\n","acc: 0.959, auc: 0.924, precision: 0.253, recall: 0.543, f1: 0.345\n","Epoch: 15 \tTraining Loss: 0.005108\n","Test Accuracy:\n","acc: 0.959, auc: 0.925, precision: 0.255, recall: 0.552, f1: 0.349\n","Epoch: 16 \tTraining Loss: 0.005103\n","Test Accuracy:\n","acc: 0.959, auc: 0.926, precision: 0.260, recall: 0.558, f1: 0.354\n","Epoch: 17 \tTraining Loss: 0.005097\n","Test Accuracy:\n","acc: 0.960, auc: 0.927, precision: 0.263, recall: 0.557, f1: 0.358\n","Epoch: 18 \tTraining Loss: 0.005092\n","Test Accuracy:\n","acc: 0.960, auc: 0.928, precision: 0.266, recall: 0.556, f1: 0.360\n","Epoch: 19 \tTraining Loss: 0.005087\n","Test Accuracy:\n","acc: 0.961, auc: 0.928, precision: 0.269, recall: 0.552, f1: 0.361\n","Epoch: 20 \tTraining Loss: 0.005082\n","Test Accuracy:\n","acc: 0.961, auc: 0.927, precision: 0.269, recall: 0.547, f1: 0.361\n","Epoch: 21 \tTraining Loss: 0.005078\n","Test Accuracy:\n","acc: 0.961, auc: 0.927, precision: 0.270, recall: 0.546, f1: 0.361\n","Epoch: 22 \tTraining Loss: 0.005073\n","Test Accuracy:\n","acc: 0.961, auc: 0.928, precision: 0.271, recall: 0.550, f1: 0.363\n","Epoch: 23 \tTraining Loss: 0.005068\n","Test Accuracy:\n","acc: 0.961, auc: 0.928, precision: 0.272, recall: 0.557, f1: 0.365\n","Epoch: 24 \tTraining Loss: 0.005064\n","Test Accuracy:\n","acc: 0.961, auc: 0.928, precision: 0.271, recall: 0.562, f1: 0.366\n","Epoch: 25 \tTraining Loss: 0.005059\n","Test Accuracy:\n","acc: 0.961, auc: 0.927, precision: 0.270, recall: 0.568, f1: 0.366\n","Epoch: 26 \tTraining Loss: 0.005055\n","Test Accuracy:\n","acc: 0.960, auc: 0.927, precision: 0.268, recall: 0.575, f1: 0.365\n","Epoch: 27 \tTraining Loss: 0.005052\n","Test Accuracy:\n","acc: 0.959, auc: 0.926, precision: 0.264, recall: 0.585, f1: 0.364\n","Epoch: 28 \tTraining Loss: 0.005051\n","Test Accuracy:\n","acc: 0.959, auc: 0.923, precision: 0.262, recall: 0.572, f1: 0.359\n","Epoch: 29 \tTraining Loss: 0.005051\n","Test Accuracy:\n","acc: 0.961, auc: 0.923, precision: 0.274, recall: 0.565, f1: 0.369\n","Epoch: 30 \tTraining Loss: 0.005045\n","Test Accuracy:\n","acc: 0.962, auc: 0.923, precision: 0.280, recall: 0.576, f1: 0.377\n","Epoch: 31 \tTraining Loss: 0.005039\n","Test Accuracy:\n","acc: 0.961, auc: 0.923, precision: 0.273, recall: 0.586, f1: 0.372\n","Epoch: 32 \tTraining Loss: 0.005034\n","Test Accuracy:\n","acc: 0.961, auc: 0.922, precision: 0.272, recall: 0.578, f1: 0.370\n","Epoch: 33 \tTraining Loss: 0.005030\n","Test Accuracy:\n","acc: 0.957, auc: 0.922, precision: 0.255, recall: 0.588, f1: 0.356\n","Epoch: 34 \tTraining Loss: 0.005024\n","Test Accuracy:\n","acc: 0.959, auc: 0.928, precision: 0.265, recall: 0.599, f1: 0.367\n","Epoch: 35 \tTraining Loss: 0.005014\n","Test Accuracy:\n","acc: 0.959, auc: 0.927, precision: 0.264, recall: 0.580, f1: 0.363\n","Epoch: 36 \tTraining Loss: 0.005011\n","Test Accuracy:\n","acc: 0.961, auc: 0.931, precision: 0.274, recall: 0.587, f1: 0.373\n","Epoch: 37 \tTraining Loss: 0.005008\n","Test Accuracy:\n","acc: 0.961, auc: 0.932, precision: 0.275, recall: 0.581, f1: 0.373\n","Epoch: 38 \tTraining Loss: 0.005005\n","Test Accuracy:\n","acc: 0.962, auc: 0.932, precision: 0.280, recall: 0.561, f1: 0.374\n","Epoch: 39 \tTraining Loss: 0.005004\n","Test Accuracy:\n","acc: 0.962, auc: 0.929, precision: 0.277, recall: 0.545, f1: 0.367\n","Epoch: 40 \tTraining Loss: 0.005000\n","Test Accuracy:\n","acc: 0.963, auc: 0.930, precision: 0.279, recall: 0.539, f1: 0.368\n","Epoch: 41 \tTraining Loss: 0.003262\n","Test Accuracy:\n","acc: 0.982, auc: 0.921, precision: 0.550, recall: 0.543, f1: 0.546\n","Epoch: 42 \tTraining Loss: 0.000101\n","Test Accuracy:\n","acc: 0.983, auc: 0.931, precision: 0.573, recall: 0.524, f1: 0.548\n","Epoch: 43 \tTraining Loss: 0.000093\n","Test Accuracy:\n","acc: 0.981, auc: 0.919, precision: 0.538, recall: 0.483, f1: 0.509\n","Epoch: 44 \tTraining Loss: 0.000082\n","Test Accuracy:\n","acc: 0.980, auc: 0.922, precision: 0.510, recall: 0.514, f1: 0.512\n","Epoch: 45 \tTraining Loss: 0.000072\n","Test Accuracy:\n","acc: 0.983, auc: 0.926, precision: 0.600, recall: 0.500, f1: 0.545\n","Epoch: 46 \tTraining Loss: 0.000065\n","Test Accuracy:\n","acc: 0.982, auc: 0.924, precision: 0.559, recall: 0.513, f1: 0.535\n","Epoch: 47 \tTraining Loss: 0.000060\n","Test Accuracy:\n","acc: 0.981, auc: 0.925, precision: 0.538, recall: 0.519, f1: 0.528\n","Epoch: 48 \tTraining Loss: 0.000051\n","Test Accuracy:\n","acc: 0.983, auc: 0.929, precision: 0.576, recall: 0.527, f1: 0.551\n","Epoch: 49 \tTraining Loss: 0.000038\n","Test Accuracy:\n","acc: 0.978, auc: 0.925, precision: 0.465, recall: 0.556, f1: 0.506\n","Epoch: 50 \tTraining Loss: 0.000013\n","Test Accuracy:\n","acc: 0.980, auc: 0.933, precision: 0.492, recall: 0.547, f1: 0.518\n"]}],"source":["# prep model for training\n","model.train()\n","   \n","# Set parameters\n","batch_size = 400\n","num_epochs = 50\n","\n","# Train the model\n","for epoch in range(num_epochs):\n","    train_loss = 0\n","    for i in range(0, len(X_train), batch_size):\n","        optimizer.zero_grad()\n","        batch_train = X_train[i:i+batch_size]\n","        batch_labels = Y_train[i:i+batch_size]\n","        output_train = model(batch_train)\n","        loss = criterion(output_train, batch_labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","        #print(f\"Epoch: {epoch+1}/{num_epochs}, Batch: {i//batch_size+1}/{len(X_train)//batch_size+1}, Training Loss: {loss.item():.4f}\")\n","    \n","    train_loss = train_loss / len(X_train)\n","    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n","    print(\"Test Accuracy:\")\n","    evaluate(model, X_test, Y_test)\n","\n","    "]},{"cell_type":"code","source":["#saved_model_path = '/content/drive/MyDrive/Colab Notebooks/Word2Vec.pth'\n","saved_model_path = 'Word2Vec.pth'\n","torch.save(model.state_dict(), saved_model_path)"],"metadata":{"id":"hmPVaFc_XL0J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0FZOOBDuVbYK"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}